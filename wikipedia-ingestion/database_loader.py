"""Database loader for timeline events.

This is the 'Load' step in the ETL pipeline. It discovers all JSON artifact
files generated by ingestion strategies and loads them into the database.

Usage:
    python database_loader.py

Environment Variables:
    ARTIFACT_DIR: Directory containing JSON artifact files (default: logs/)
    ARTIFACT_PATTERN: Glob pattern for artifact files (default: events_*.json)
    DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD: Database connection settings
    INGEST_TARGET_TABLE: Target table name (default: historical_events)
    LOADER_MODE: Load mode - 'replace' (clear all) or 'upsert' (delete by URL then insert)
                 Default: 'replace'

The loader:
1. Discovers all JSON artifact files matching the pattern
2. Validates each artifact's schema
3. Deduplicates events across all artifacts
4. In 'replace' mode: Clears existing data, then inserts
5. In 'upsert' mode: Collects URLs, deletes matching events, then inserts
6. Generates a load summary report and error log
"""

from __future__ import annotations

import json
import os
import re
from datetime import datetime
from pathlib import Path
from typing import Any

try:
    from .ingestion_common import (
        clear_previously_ingested,
        connect_db,
        insert_event,
        log_error,
        log_info,
    )
    from .strategy_base import validate_event_dict
except ImportError:  # pragma: no cover
    from ingestion_common import (
        clear_previously_ingested,
        connect_db,
        insert_event,
        log_error,
        log_info,
    )
    from strategy_base import validate_event_dict


def discover_artifact_files(artifact_dir: Path, pattern: str = "events_*.json") -> list[Path]:
    """Discover all artifact JSON files in the directory.
    
    Args:
        artifact_dir: Directory to search for artifacts
        pattern: Glob pattern for artifact files
        
    Returns:
        List of Path objects for artifact files, sorted by name
    """
    if not artifact_dir.exists():
        log_error(f"Artifact directory does not exist: {artifact_dir}")
        return []
    
    artifacts = list(artifact_dir.glob(pattern))
    artifacts.sort()  # Process in consistent order
    
    log_info(f"Discovered {len(artifacts)} artifact file(s) in {artifact_dir}")
    for artifact in artifacts:
        log_info(f"  - {artifact.name}")
    
    return artifacts


def load_artifact(artifact_path: Path, errors: list[dict]) -> dict | None:
    """Load and validate a single artifact file.
    
    Args:
        artifact_path: Path to artifact JSON file
        errors: List to collect error details
        
    Returns:
        Artifact data dict, or None if loading/validation fails
    """
    try:
        with open(artifact_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        # Validate artifact structure
        if not isinstance(data, dict):
            error_detail = {
                "type": "artifact_validation",
                "file": artifact_path.name,
                "error": f"expected dict, got {type(data)}",
                "timestamp": datetime.utcnow().isoformat() + "Z"
            }
            errors.append(error_detail)
            log_error(f"Invalid artifact {artifact_path.name}: expected dict, got {type(data)}")
            return None
        
        required_fields = {"strategy", "run_id", "event_count", "events"}
        missing = required_fields - set(data.keys())
        if missing:
            error_detail = {
                "type": "artifact_validation",
                "file": artifact_path.name,
                "error": f"missing fields {missing}",
                "timestamp": datetime.utcnow().isoformat() + "Z"
            }
            errors.append(error_detail)
            log_error(f"Invalid artifact {artifact_path.name}: missing fields {missing}")
            return None
        
        if not isinstance(data["events"], list):
            error_detail = {
                "type": "artifact_validation",
                "file": artifact_path.name,
                "error": "events must be a list",
                "timestamp": datetime.utcnow().isoformat() + "Z"
            }
            errors.append(error_detail)
            log_error(f"Invalid artifact {artifact_path.name}: events must be a list")
            return None
        
        log_info(
            f"Loaded artifact {artifact_path.name}: "
            f"{data['event_count']} events from {data['strategy']}"
        )
        
        return data
        
    except json.JSONDecodeError as e:
        error_detail = {
            "type": "json_parse",
            "file": artifact_path.name,
            "error": str(e),
            "timestamp": datetime.utcnow().isoformat() + "Z"
        }
        errors.append(error_detail)
        log_error(f"Failed to parse {artifact_path.name}: {e}")
        return None
    except Exception as e:
        error_detail = {
            "type": "artifact_load",
            "file": artifact_path.name,
            "error": str(e),
            "timestamp": datetime.utcnow().isoformat() + "Z"
        }
        errors.append(error_detail)
        log_error(f"Failed to load {artifact_path.name}: {e}")
        return None


def collect_all_events(artifacts: list[dict], errors: list[dict]) -> tuple[list[dict], dict]:
    """Collect and validate events from all artifacts.
    
    Args:
        artifacts: List of artifact data dicts
        errors: List to collect error details
        
    Returns:
        Tuple of (valid_events, stats_dict)
    """
    all_events = []
    stats = {
        "total_events": 0,
        "valid_events": 0,
        "invalid_events": 0,
        "by_strategy": {}
    }
    
    for artifact in artifacts:
        strategy = artifact.get("strategy", "unknown")
        events = artifact.get("events", [])
        
        strategy_valid = 0
        strategy_invalid = 0
        
        for i, event in enumerate(events):
            stats["total_events"] += 1
            
            # Validate event schema
            is_valid, error_msg = validate_event_dict(event)
            
            if not is_valid:
                error_detail = {
                    "type": "event_validation",
                    "strategy": strategy,
                    "event_index": i,
                    "title": event.get("title", "(no title)"),
                    "precision": event.get("precision", "(no precision)"),
                    "error": error_msg,
                    "timestamp": datetime.utcnow().isoformat() + "Z"
                }
                errors.append(error_detail)
                log_error(
                    f"Invalid event from {strategy} at index {i}: {error_msg}. "
                    f"Event: {event.get('title', '(no title)')}. "
                    f"Precision is {event.get('precision', '(no precision)')}"
                )
                stats["invalid_events"] += 1
                strategy_invalid += 1
                continue
            
            all_events.append(event)
            stats["valid_events"] += 1
            strategy_valid += 1
        
        stats["by_strategy"][strategy] = {
            "valid": strategy_valid,
            "invalid": strategy_invalid
        }
    
    log_info(
        f"Collected {stats['valid_events']} valid events "
        f"({stats['invalid_events']} invalid)"
    )
    
    return all_events, stats


def deduplicate_events(events: list[dict]) -> tuple[list[dict], int]:
    """Deduplicate events using normalized title + dates as key.
    
    Args:
        events: List of events (potentially with duplicates)
        
    Returns:
        Tuple of (deduplicated_events, duplicate_count)
    """
    seen_keys: set[tuple] = set()
    deduplicated = []
    duplicate_count = 0
    
    for event in events:
        # Normalize title
        normalized_title = re.sub(r"\s+", " ", event["title"].strip().lower())
        
        event_key = (
            normalized_title,
            int(event["start_year"]),
            int(event["end_year"]),
            bool(event["is_bc_start"]),
        )
        
        if event_key in seen_keys:
            duplicate_count += 1
            continue
        
        seen_keys.add(event_key)
        deduplicated.append(event)
    
    if duplicate_count > 0:
        log_info(f"Deduplicated {duplicate_count} duplicate events")
    
    log_info(f"Final event count after deduplication: {len(deduplicated)}")
    
    return deduplicated, duplicate_count


def collect_urls_from_events(events: list[dict]) -> set[str]:
    """Collect all unique URLs from events.
    
    Args:
        events: List of events
        
    Returns:
        Set of unique URLs
    """
    urls = {event.get("url") for event in events if event.get("url")}
    log_info(f"Collected {len(urls)} unique URLs from events")
    return urls


def delete_events_by_urls(conn, urls: set[str]) -> int:
    """Delete events from database that match the given URLs.
    
    Args:
        conn: Database connection
        urls: Set of URLs to delete
        
    Returns:
        Number of events deleted
    """
    if not urls:
        log_info("No URLs to delete")
        return 0
    
    log_info(f"Deleting events matching {len(urls)} URLs...")
    
    # Get target table name
    target_table = os.getenv("INGEST_TARGET_TABLE", "historical_events")
    
    cursor = conn.cursor()
    try:
        # Use ANY to delete all matching URLs in one query
        cursor.execute(
            f"DELETE FROM {target_table} WHERE wikipedia_url = ANY(%s) RETURNING id;",
            (list(urls),)
        )
        deleted_count = cursor.rowcount
        conn.commit()
        
        log_info(f"Deleted {deleted_count} events from database")
        return deleted_count
        
    except Exception as e:
        conn.rollback()
        log_error(f"Failed to delete events by URL: {e}")
        raise
    finally:
        cursor.close()


def insert_events_to_db(conn, events: list[dict], errors: list[dict]) -> tuple[int, int]:
    """Insert events to database.
    
    Args:
        conn: Database connection
        events: List of deduplicated events to insert
        errors: List to collect error details
        
    Returns:
        Tuple of (inserted_count, failed_count)
    """
    log_info(f"Inserting {len(events)} events to database...")
    
    inserted_count = 0
    failed_count = 0
    
    for i, event in enumerate(events):
        try:
            # Extract category before inserting (insert_event expects it as separate param)
            category_value = event.pop("category", None)
            
            if insert_event(conn, event, category=category_value):
                inserted_count += 1
            else:
                error_detail = {
                    "type": "insert_failed",
                    "event_index": i,
                    "title": event.get("title", "(no title)"),
                    "error": "insert_event returned False",
                    "timestamp": datetime.utcnow().isoformat() + "Z"
                }
                errors.append(error_detail)
                failed_count += 1
                
        except Exception as e:
            error_detail = {
                "type": "insert_exception",
                "event_index": i,
                "title": event.get("title", "(no title)"),
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat() + "Z"
            }
            errors.append(error_detail)
            log_error(f"Failed to insert event at index {i} ('{event.get('title', '(no title)')}'): {e}")
            failed_count += 1
            continue
    
    log_info(
        f"Database insertion complete: "
        f"{inserted_count} inserted, {failed_count} failed"
    )
    
    return inserted_count, failed_count


def write_error_log(artifact_dir: Path, errors: list[dict]) -> None:
    """Write collected errors to a JSON error log file.
    
    Args:
        artifact_dir: Directory where artifacts are located
        errors: List of error details
    """
    if not errors:
        log_info("No errors to log")
        return
    
    error_log = {
        "error_log_timestamp_utc": datetime.utcnow().isoformat() + "Z",
        "total_errors": len(errors),
        "errors": errors
    }
    
    error_log_path = artifact_dir / f"load_errors_{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}.json"
    
    try:
        with open(error_log_path, "w", encoding="utf-8") as f:
            json.dump(error_log, f, indent=2, ensure_ascii=False)
            f.write("\n")
        log_info(f"Wrote error log with {len(errors)} errors: {error_log_path}")
    except Exception as e:
        log_error(f"Failed to write error log: {e}")


def generate_load_report(
    artifact_dir: Path,
    artifacts_loaded: int,
    stats: dict,
    duplicate_count: int,
    deleted_count: int,
    inserted_count: int,
    failed_count: int,
    mode: str
) -> None:
    """Generate a summary report of the load operation.
    
    Args:
        artifact_dir: Directory where artifacts were loaded from
        artifacts_loaded: Number of artifact files processed
        stats: Statistics from event collection
        duplicate_count: Number of duplicate events removed
        deleted_count: Number of events deleted (upsert mode only)
        inserted_count: Number of events inserted successfully
        failed_count: Number of events that failed to insert
        mode: Load mode ('replace' or 'upsert')
    """
    report = {
        "load_timestamp_utc": datetime.utcnow().isoformat() + "Z",
        "load_mode": mode,
        "artifact_directory": str(artifact_dir),
        "artifacts_loaded": artifacts_loaded,
        "events_collected": stats["total_events"],
        "events_valid": stats["valid_events"],
        "events_invalid": stats["invalid_events"],
        "duplicates_removed": duplicate_count,
        "events_deleted": deleted_count,
        "events_inserted": inserted_count,
        "events_failed": failed_count,
        "by_strategy": stats["by_strategy"]
    }
    
    report_path = artifact_dir / f"load_report_{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}.json"
    
    try:
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
            f.write("\n")
        log_info(f"Wrote load report: {report_path}")
    except Exception as e:
        log_error(f"Failed to write load report: {e}")
    
    # Also log summary to console
    log_info("=" * 60)
    log_info("LOAD SUMMARY")
    log_info("=" * 60)
    log_info(f"Load mode:           {mode}")
    log_info(f"Artifacts processed: {artifacts_loaded}")
    log_info(f"Events collected:    {stats['total_events']}")
    log_info(f"Valid events:        {stats['valid_events']}")
    log_info(f"Invalid events:      {stats['invalid_events']}")
    log_info(f"Duplicates removed:  {duplicate_count}")
    if mode == "upsert":
        log_info(f"Events deleted:      {deleted_count}")
    log_info(f"Events inserted:     {inserted_count}")
    log_info(f"Events failed:       {failed_count}")
    log_info("=" * 60)
    
    for strategy, counts in stats["by_strategy"].items():
        log_info(f"  {strategy}: {counts['valid']} valid, {counts['invalid']} invalid")


def load_artifacts_to_database(errors: list[dict[str, Any]]) -> None:
    """Main function to load all artifacts to database."""
    
    # Configuration
    artifact_dir = Path(os.getenv("ARTIFACT_DIR", "logs"))
    artifact_pattern = os.getenv("ARTIFACT_PATTERN", "*.json")
    mode = os.getenv("LOADER_MODE", "replace").lower()
    
    if mode not in {"replace", "upsert"}:
        log_error(f"Invalid LOADER_MODE: {mode}. Must be 'replace' or 'upsert'")
        return
    
    log_info("Starting database loader...")
    log_info(f"Artifact directory: {artifact_dir}")
    log_info(f"Artifact pattern:   {artifact_pattern}")
    log_info(f"Load mode:          {mode}")
    
    # Initialize error collection is now done in main()
    
    # Discover artifact files
    artifact_paths = discover_artifact_files(artifact_dir, artifact_pattern)
    
    if not artifact_paths:
        log_info("No artifact files found. Nothing to load.")
        return
    
    # Load all artifacts
    artifacts = []
    for artifact_path in artifact_paths:
        artifact_data = load_artifact(artifact_path, errors)
        if artifact_data:
            artifacts.append(artifact_data)
    
    if not artifacts:
        log_error("No valid artifacts loaded. Aborting.")
        write_error_log(artifact_dir, errors)
        return
    
    # Collect and validate events
    all_events, stats = collect_all_events(artifacts, errors)
    
    if not all_events:
        log_error("No valid events collected. Aborting.")
        write_error_log(artifact_dir, errors)
        return
    
    # Deduplicate events
    deduplicated_events, duplicate_count = deduplicate_events(all_events)
    
    # Connect to database
    conn = connect_db()
    deleted_count = 0
    
    try:
        if mode == "replace":
            # Clear all existing data
            log_info("Mode: REPLACE - clearing all existing data")
            clear_previously_ingested(conn)
            
        elif mode == "upsert":
            # Two-pass approach: collect URLs, delete, then insert
            log_info("Mode: UPSERT - deleting by URL then inserting")
            
            # Pass 1: Collect all URLs from artifacts
            urls_to_delete = collect_urls_from_events(deduplicated_events)
            
            # Pass 2: Delete events matching those URLs
            deleted_count = delete_events_by_urls(conn, urls_to_delete)
        
        # Insert events (same for both modes)
        inserted_count, failed_count = insert_events_to_db(conn, deduplicated_events, errors)
        
        # Generate report
        generate_load_report(
            artifact_dir,
            len(artifacts),
            stats,
            duplicate_count,
            deleted_count,
            inserted_count,
            failed_count,
            mode
        )
        
        # Write error log
        write_error_log(artifact_dir, errors)
        
    finally:
        conn.close()
    
    log_info("Database loader complete")


def main() -> None:
    """Entry point for database loader."""
    errors: list[dict[str, Any]] = []
    artifact_dir = Path(os.getenv("ARTIFACT_DIR", "logs"))
    
    try:
        load_artifacts_to_database(errors)
    except Exception as e:
        log_error(f"Database loader failed: {e}")
        import traceback
        log_error(traceback.format_exc())
        # Write error log even on failure
        write_error_log(artifact_dir, errors)
        raise


if __name__ == "__main__":
    log_info("Starting database loader...")
    main()
